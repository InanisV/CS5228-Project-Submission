{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"xgboost_regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOt5C0SrW5BI8s32sH7oHNY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g4FBRi0j613v"},"source":["## XGBoost Regression\n","We use xgboost library and gridsearch cv to search for the best parameters."]},{"cell_type":"code","metadata":{"id":"osc9MDwn7Da8"},"source":["import xgboost as xgb\n","from sklearn import metrics\n","from xgboost import plot_importance\n","from sklearn import preprocessing\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import hashlib\n","from xgboost import plot_importance, plot_tree\n","import math\n","from matplotlib import pyplot\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvL6cwWX7StO"},"source":["# index_ = [\"years of warranty\",\"better loan offer\",\"well maintained\",\"low fuel consumption\",\"reg_date\",\"power\",\"engine_cap\",\"mileage\",\"no_of_owners\",\"depreciation\",\"coe\",\"dereg_value\",\"omv\",\"arf\",\"type_of_vehicle_bus/mini bus\",\"type_of_vehicle_hatchback\",\"type_of_vehicle_luxury sedan\",\"type_of_vehicle_mid-sized sedan\",\"type_of_vehicle_mpv\",\"type_of_vehicle_others\",\"type_of_vehicle_sports car\",\"type_of_vehicle_stationwagon\",\"type_of_vehicle_suv\",\"type_of_vehicle_truck\",\"type_of_vehicle_van\",\"fuel_type_diesel\",\"fuel_type_electric\",\"fuel_type_petrol\",\"fuel_type_petrol-electric\",\"transmission\",\"model_price\", \"is_new\"]\n","index_ = [\"low fuel consumption\", \"well maintained\", \"better loan offer\",\"reg_date\",\"power\",\"engine_cap\",\"mileage\",\"no_of_owners\",\"depreciation\",\"coe\",\"dereg_value\",\"omv\",\"arf\",\"model_price\", \"is_new\"]\n","#\n","temp_idx = [\"acc_{}\".format(i) for i in range(1, 12)]\n","temp_idx.extend(index_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t-gbViJs7VmW"},"source":["#load the training set\n","def process_training_set(make_encoder, model_encoder):\n","    ori_data = pd.read_csv(\"../data/train_v6.csv\")\n","    # ori_data['model'] = ori_data['model'].map(lambda x: x.lower())\n","    # ori_data['make'] = ori_data['make'].map(lambda x: x.lower())\n","    # ori_data['model'] = model_encoder.transform(list(ori_data['model'].values))\n","    # ori_data['make'] = make_encoder.transform(list(ori_data['make'].values))\n","    X = []\n","    y = []\n","    cnt = 0\n","    for index, row in ori_data.iterrows():\n","        if cnt == 0:\n","            cnt += 1\n","            continue\n","        temp_row = []\n","        # print(row[\"accessories_vectors\"].replace('[', '').replace(']', '').replace(' ', '').split(','))\n","        acc_list = [1 if str(i) in row[\"accessories_vectors\"].replace('[', '').replace(']', '').replace(' ', '').split(',') else 0 for i in range(1, 12)]\n","        temp_row.extend(acc_list)\n","        temp_row.extend(row[index_].tolist())\n","        # make_ = row[\"make\"].lower()\n","        # model_ = row[\"model\"].lower()\n","        # m = hashlib.md5()\n","        # m.update(make_.encode(\"utf-8\"))\n","        # mo = hashlib.md5()\n","        # mo.update(model_.encode(\"utf-8\"))\n","        # temp_row.append(int(m.hexdigest(), 16) % 500)\n","        # temp_row.append(int(mo.hexdigest(), 16) % 2000)\n","        # print(int(m.hexdigest(), 16))\n","        X.append(temp_row)\n","        y.append(float(row['price']))\n","    return X, y\n","    # train_features = pd.DataFrame(X)\n","    # train_features.to_csv(\"../data/train_features.csv\")\n","    # train_labels = pd.DataFrame(y)\n","    # train_labels.to_csv(\"../data/train_labels.csv\")\n","        # print(row[\"accessories_vectors\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSxC7NWDHWWC"},"source":["def process_test_set(make_encoder, model_encoder):\n","    ori_data = pd.read_csv(\"../data/test_v6.csv\")\n","    # ori_data['model'] = ori_data['model'].map(lambda x: x.lower())\n","    # ori_data['make'] = ori_data['make'].map(lambda x: x.lower())\n","    # ori_data['model'] = model_encoder.transform(list(ori_data['model'].values))\n","    # ori_data['make'] = make_encoder.transform(list(ori_data['make'].values))\n","    X = []\n","    id_list = []\n","    for index, row in ori_data.iterrows():\n","        temp_row = []\n","        # print(row[\"accessories_vectors\"].replace('[', '').replace(']', '').replace(' ', '').split(','))\n","        acc_list = [1 if str(i) in row[\"accessories_vectors\"].replace('[', '').replace(']', '').replace(' ', '').split(',') else 0 for i in range(1, 12)]\n","        temp_row.extend(acc_list)\n","        temp_row.extend(row[index_].tolist())\n","        # make_ = row[\"make\"].lower()\n","        # model_ = row[\"model\"].lower()\n","        # m = hashlib.md5()\n","        # m.update(make_.encode(\"utf-8\"))\n","        # mo = hashlib.md5()\n","        # mo.update(model_.encode(\"utf-8\"))\n","        # temp_row.append(int(m.hexdigest(), 16) % 500)\n","        # temp_row.append(int(mo.hexdigest(), 16) % 2000)\n","        # print(int(m.hexdigest(), 16))\n","        X.append(temp_row)\n","        id_list.append(row[\"listing_id\"])\n","    return X, id_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhwLDweJHbSZ"},"source":["## Generate the datasets for training\n"]},{"cell_type":"code","metadata":{"id":"xHgvu3XlHgii"},"source":["make_encoder, model_encoder = 0, 0\n","test_X, id_list = process_test_set(make_encoder, model_encoder)\n","x, y = process_training_set(make_encoder, model_encoder)\n","x, y = np.array(x), np.array(y)\n","print('x,y shape', np.array(x).shape, np.array(y).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NS2rD34IdJp"},"source":["## Generate the model and predict the result for test set"]},{"cell_type":"code","metadata":{"id":"pmYQypMxIjSh"},"source":["xgb_model = xgb.XGBRegressor(learning_rate=0.1,\n","        max_depth=5,\n","        min_child_weight=3,\n","        gamma=0,\n","        seed=27, tree_method='gpu_hist', subsample = 0.6, colsample_bytree=0.6, n_estimators=600, reg_alpha=0.1)\n","xgb_model.fit(x, y)\n","xgb.XGBRegressor.feature_names = temp_idx\n","xgb_model.feature_names = temp_idx\n","plot_importance(xgb_model)\n","# plot_tree(xgb_model)\n","# pyplot.savefig(\"xgboost_tree\")\n","# pyplot.show()\n","# print(xgb_model.feature_importances_)\n","# pyplot.bar(range(len(xgb_model.feature_importances_)), xgb_model.feature_importances_)\n","pyplot.show()\n","y_pred = xgb_model.predict(np.array(test_X))\n","y_pred = np.around(y_pred, 1)\n","y_pred = y_pred.tolist()\n","t_id_list = [i for i in range(len(y_pred))]\n","res_df = pd.DataFrame({\"Id\":t_id_list, \"Predicted\":y_pred})\n","# res_df = res_df.drop_duplicates(subset=[\"Id\"], keep=\"first\")\n","res_df.to_csv(\"../data/submit.csv\", index=None)\n","\n","print(\"\\tXGBoost模型：\", cross_val_score(xgb_model, x, y, cv=5, scoring=\"neg_root_mean_squared_error\").mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIK3iwvSIt_I"},"source":["## Parameter adjustment par"]},{"cell_type":"code","metadata":{"id":"7ykqhP3sIzDQ"},"source":["# subsample colsample_bytree\n","from sklearn.model_selection import GridSearchCV\n","\n","param_test3 = {\n","    'subsample':[i/10.0 for i in range(6,10)],\n","'colsample_bytree':[i/10.0 for i in range(6,10)]\n","}\n","gsearch = GridSearchCV(\n","    estimator=xgb.XGBRegressor(\n","        learning_rate=0.1,\n","        max_depth=9,\n","        min_child_weight=3,\n","        n_estimators=500,\n","        gamma=0,\n","        seed=27, tree_method='gpu_hist'),\n","    param_grid=param_test3,\n","    scoring='neg_root_mean_squared_error',\n","    cv=5)\n","gsearch.fit(x, y)\n","print('subsample_colsample_bytree------------------')\n","print('gsearch1.grid_scores_', gsearch.cv_results_['mean_test_score'])\n","print(gsearch.cv_results_['params'])\n","print('gsearch1.best_params_', gsearch.best_params_)\n","print('gsearch1.best_score_', gsearch.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5aZBeOrJB9_"},"source":["# reg_alpha \n","from sklearn.model_selection import GridSearchCV\n","\n","param_test3 = {\n","    # 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n","'reg_alpha':[0.1, 1]}\n","gsearch = GridSearchCV(\n","    estimator=xgb.XGBRegressor(\n","        learning_rate=0.1,\n","        max_depth=5,\n","        min_child_weight=3,\n","        gamma=0,\n","        seed=27, tree_method='gpu_hist', subsample = 0.6, colsample_bytree=0.6, n_estimators=600),\n","    param_grid=param_test3,\n","    scoring='neg_root_mean_squared_error',\n","    cv=5)\n","gsearch.fit(x, y)\n","print('reg_alpha------------------')\n","print('gsearch1.grid_scores_', gsearch.cv_results_['mean_test_score'])\n","print(gsearch.cv_results_['params'])\n","print('gsearch1.best_params_', gsearch.best_params_)\n","print('gsearch1.best_score_', gsearch.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPPEA3JvJFjw"},"source":["# max_depth 与 min_child_weight\n","param_test1 = {\n","    'max_depth': [i for i in range(3, 10, 2)],\n","    'min_child_weight': [i for i in range(1, 6, 2)]\n","}\n","# from sklearn import svm, grid_search, datasets\n","# from sklearn import grid_search\n","from sklearn.model_selection import GridSearchCV\n","gsearch = GridSearchCV(\n","    estimator=xgb.XGBRegressor(\n","        learning_rate=0.1,booster=\"gbtree\",objective=\"reg:squarederror\",n_estimators=500,reg_alpha=100,\n","        seed=27,tree_method='gpu_hist', colsample_bytree=0.6, subsample=0.6),\n","    param_grid=param_test1,\n","    scoring='neg_root_mean_squared_error',\n","    cv=5)\n","gsearch.fit(x, y)\n","print('max_depth_min_child_weight')\n","print('gsearch1.grid_scores_', gsearch.cv_results_['mean_test_score'])\n","print(gsearch.cv_results_['params'])\n","print('gsearch1.best_params_', gsearch.best_params_)\n","print('gsearch1.best_score_', gsearch.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9I5cMYdJKfg"},"source":["# gamma\n","param_test3 = {'gamma': [i / 10.0 for i in range(0, 20, 4)]}\n","from sklearn.model_selection import GridSearchCV\n","\n","gsearch = GridSearchCV(\n","    estimator=xgb.XGBRegressor(\n","        learning_rate=0.1, booster=\"gbtree\", objective=\"reg:squarederror\", n_estimators=600,\n","        seed=27, tree_method='gpu_hist', max_depth=5, min_child_weight=3),\n","    param_grid=param_test3,\n","    scoring='neg_root_mean_squared_error',\n","    cv=5)\n","gsearch.fit(x, y)\n","print('max_depth_min_child_weight')\n","print('gsearch1.grid_scores_', gsearch.cv_results_['mean_test_score'])\n","print(gsearch.cv_results_['params'])\n","print('gsearch1.best_params_', gsearch.best_params_)\n","print('gsearch1.best_score_', gsearch.best_score_)"],"execution_count":null,"outputs":[]}]}