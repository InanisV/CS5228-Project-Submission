{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"accessories_preprocess.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1yf2yUAkcetJh+fAeRe4J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"55BsxoKokx6t"},"source":["This note book is for searching for the accessories that we listed in the dataset and generate a vector marking whether the car as such accessories for every car."]},{"cell_type":"code","metadata":{"id":"TxPDHbcalJqS"},"source":["import torch\n","from transformers import BertTokenizer, BertModel\n","import logging\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import re\n","import pandas as pd\n","from string import digits\n","from multiprocessing import Pool\n","import os, time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8GtOAeSlMfc"},"source":["def load_csv():\n","    #load the original csv and fetch(id, accessories)\n","    ori_file = \"../data/train.csv\"\n","    # ori_file = \"test.csv\"\n","    df_ = pd.read_csv(ori_file)\n","    target_df = df_[['listing_id', 'accessories']]\n","    return target_df['listing_id'].values.tolist(), target_df['accessories'].values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X6ujQV_4lP7M"},"source":["#search if the word is a substring of the latter string\n","def judge_substring(sub_str, tar_str):\n","    flag = True\n","    sub_str = sub_str.split(' ')\n","    for str_ in sub_str:\n","        if str_ not in tar_str:\n","            flag = False\n","            break\n","    return flag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-FpGuJB-laCt"},"source":["#given a text, use bert to generate the embedded vector\n","def bert_embed(text, model, tokenizer):\n","    marked_text = \"[CLS] \" + text + \" [SEP]\"\n","    tokenized_text = tokenizer.tokenize(marked_text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","    segments_ids = [1] * len(tokenized_text)\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    tokens_tensor = tokens_tensor.to('cuda')\n","    segments_tensors = torch.tensor([segments_ids])\n","    segments_tensors = segments_tensors.to('cuda')\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, segments_tensors)\n","        hidden_states = outputs[2]\n","        token_vecs = hidden_states[-2][0]\n","        sentence_embedding = torch.mean(token_vecs, dim=0)\n","        return sentence_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XLaap6-0vOe"},"source":["# judge if the sentence contains certain items using cosine similarity and substring_judge\n","def compare_items(text, item_list, vector_):\n","    # print(\"matched:\")\n","    res_list = []\n","    threshold = 0.5\n","    for tp in item_list:\n","        target_vector = tp[2]\n","        sim = torch.cosine_similarity(vector_.reshape(1, -1), target_vector.reshape(1, -1))\n","        if int(sim) > threshold or judge_substring(tp[1], text):\n","            res_list.append(tp[0])\n","            # print(tp[1])\n","    res_list = list(set(res_list))\n","    return res_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MFoeCoM1JVW"},"source":["#call bert embeding function to generate the vector\n","def generate_vector(item_list, source_df, model, tokenizer):\n","    result = []\n","    remove_digits = str.maketrans('', '', digits)\n","    for index__, row in tqdm(source_df.iterrows()):\n","        # print(\"subprocess {} has finished {}/{} itr\".format(num, cnt, total_length))\n","        # cnt += 1\n","        id_ = row['listing_id']\n","        text_ = str(row['accessories'])\n","        text_list = text_.replace('.', ',').split(',')\n","        temp_res = []\n","        for text in text_list:\n","            text = text.strip().replace('/', ' ').translate(remove_digits).lower()\n","            # print('=' * 20)\n","            # print(\"original_text: \" + text)\n","            sentence_embedding = bert_embed(text, model, tokenizer)\n","            item_idxs = compare_items(text, item_list, sentence_embedding)\n","            temp_res.extend(item_idxs)\n","        result.append((id_, temp_res))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnBjnAxx1V99"},"source":["#load the item list we have\n","def item_list_generate(model, tokenizer):\n","    item_file = \"../data/items.csv\"\n","    item_df = pd.read_csv(item_file)\n","    id_list = item_df['Id'].values.tolist()\n","    item_list = item_df['Items'].values.tolist()\n","    res_list = []\n","    for i in range(len(item_list)):\n","        idx = id_list[i]\n","        items = item_list[i].split(',')\n","        for item in items:\n","            item = item.strip().lower()\n","            embd = bert_embed(item, model, tokenizer)\n","            res_list.append(tuple([idx, item, embd]))\n","    return res_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoeNN3f-1d_M"},"source":["#main function, call all the functions above and generate the result csv\n","id__, acc__ = load_csv()\n","print(len(id__))\n","acc_df = pd.DataFrame({'listing_id': id__, 'accessories': acc__})\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased',\n","                                    output_hidden_states=True,  # Whether the model returns all hidden-states.\n","                                    )\n","model.to('cuda')\n","item_list = item_list_generate(model, tokenizer)\n","total_dict = {}\n","print(acc_df.shape)\n","# temp_ = acc_df.drop_duplicates(subset=[\"listing_id\"], keep=\"first\")\n","# print(temp_.shape)\n","total_dict = generate_vector(item_list, acc_df, model, tokenizer)\n","ids_ = [i[0] for i in total_dict]\n","vectors = [i[1] for i in total_dict]\n","data_ = {'listing_id': ids_, 'accessories_vectors': vectors}\n","res_df = pd.DataFrame(data_)\n","res_df.to_csv(\"../data/embed_accessories.csv\")\n"],"execution_count":null,"outputs":[]}]}